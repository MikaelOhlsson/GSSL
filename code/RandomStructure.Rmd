---
title: "Random Networks"
author: "Mikael Ohlsson"
date: "2024-01-30"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
theme_set(theme_bw())
theme_update(panel.grid = element_blank(),
             strip.background = element_rect(fill = "white", colour = "black"))
```

## Generate random networks

Using network size and connectance of each respective food web in the study. Currently, networks are pre-generated in $data/random/$ and this part of the code will not run. To re-run (generating new random networks), change $projfolder$ (or delete the $data/random/$ folder) and run again.  

```{r vars}
projfolder <- "random"

#If needed, create necessary path
if(!projfolder %in% list.dirs("../data", full.names = F)) dir.create(paste0("../data/", projfolder))

#Unless already generated, start making food webs
if(length(list.files(paste0("../data/", projfolder))) < 1) {
  for(n in c(100,200)){ # Network sizes n
    for(i in 1:5){ # Number of random matrices per size
      matrix(sample(c(rep(0, round((1-0.086)*(n*n))), # Decides connectance
                      rep(1, round(0.086*(n*n)))),
                    (n*n), replace = F), nrow = n, ncol = n) %>%
        write.table(file = paste0("../data/", projfolder, "/N", n, "_c0086_i", i,".txt"),
                    row.names = F, col.names = F)
    }
  }
}

```

## Run the group model

Currently the data is already provided in $results/random/$ and the group model will not be executed. If you want to run it, change $projfolder$ above (or delete the $results/random/$ folder) and run the code again.

NOTE: The group model first needs to be compiled using the $make$ command in the $../GroupModelAlorithm$ folder.

It is set up to run multiple iterations in parallel (number of cores - 2). This will still take approximately 10 hours.

```{r groupmodel}
# Packages for parallelizing
library(foreach)
library(doParallel)

# First, checking if result files already exist. If not, running the group model. Time consuming unless run in parallel.
if(length(list.files(paste0("../results/", projfolder))) < 1){
  setwd("../GroupModelAlgorithm")
  adjfolder <- paste0("../data/", projfolder)
  
# Grab filenames and paths 
  netlist <- list.files(path = adjfolder, pattern = "*.txt$", full.names = T)
  netnames <- list.files(path = adjfolder, pattern = "*.txt$", full.names = F)
  
  timereq <- function(s){ # If parallelizing on a cluster as separate jobs, not currently used here
    {
      if(s == 100) t <- "00:30:00"
      else t <- "02:00:00"
    }
    return(t)
  }
  
  
  stepreq <- function(s){ # MCMC steps depending on size of network
    # This set up should take about 100 core hours (i.e. divide by number of cores to get an approximate real time)
    {
      if(s == 100) sr <- 200000
      else sr <- 200000 # more steps for the larger networks
    }
    return(sr)
  }
  
  netmeta <- as_tibble(netnames) %>% 
    separate(value, into =c("size", "rest"), sep ="_c0086_i") %>%
    separate(rest, into =c("iter", "rest"), sep =".txt") %>%
    mutate(size = as.numeric(substr(size, 2,4)),
           iter = as.numeric(iter)) %>% 
    dplyr::select(-rest) %>%
    mutate(time = unlist(map(size, timereq))) %>%
    mutate(step = unlist(map(size, stepreq)))
    
  #Number of MCMC chains
  chains <- 20
  
  #Number of runs per network (runs with identical solutions will overwrite each other)
  runs <- 100
  
  #Max number of groups per network
  groups <- 20 
  
  # Set up parallelizing
  CCount <- detectCores()-2
  myCluster <- makeCluster(CCount, type = "PSOCK")  
  registerDoParallel(myCluster)
  
  for(i in 1:length(netlist)){
    foreach(x=1:runs) %dopar% {
      r <- as.character(floor(runif(1, min=100001, max=1000000))) #random seed
      exec <- paste("./FindGroups", netmeta[i,1], netlist[i], r, netmeta[i,4], chains, groups, 0) # ./FindGroups NodeSize AdjMatPath RandomSeed MCMCSteps MCMCChains MaxGroups DegreeCorr
    #AFTER groups, 0 flag to deactivate degree correction  OR
    #1, 1, 1 flags to activate degree correction
    system(exec) #> paste0(netnames[i], "_out.txt")
    }
  }
  stopCluster(myCluster)
  
  setwd("../code")
  
  # Group model output data uses the same folder as source food webs. Hence, after finishing, moving group model result files to result folder:
  if(!projfolder %in% list.dirs("../results", full.names = F)) dir.create(paste0("../results/", projfolder))
  
  movefiles <- list.files(paste0("../data/", projfolder), pattern = ".txt-G")
  if(length(movefiles > 1)) {
    targetlocation <- paste0("../results/", projfolder,"/", movefiles)
    file.rename(from = paste0("../data/", projfolder,"/", movefiles), to = targetlocation)
  }
}
```


## Collect data from group model runs

```{r gatherdata}

LETTERS702 <- c(LETTERS, sapply(LETTERS, function(x) paste0(x, LETTERS))) # "Species names"

fetchrand <- function(r){
    scan(file = paste0("../results/",projfolder,"/",r)) + 1
  }

randomdf <- list.files(paste0("../results/",projfolder,"/"), pattern = "*.txt-G")  %>% 
    as_tibble() %>% 
    separate(col = value, into = c("N", "rest"),
             sep = c("_c"), remove = F) %>%
    mutate(N = as.numeric(substr(N, 2,4))) %>%
    separate(col = rest, into = c("c", "rest"),
             sep = c("_i")) %>%
    separate(col = rest, into = c("iter", "rest"),
             sep = c("_r")) %>%
    separate(col = rest, into = c("run", "marginal"),
             sep = c(".txt-G-20-DC-0-alpha-NA-beta-NA-Marginal")) %>%
    mutate(iter = as.numeric(iter),
           run = as.numeric(run),
           marginal = as.numeric(marginal)) %>%
    mutate(group = map(value, fetchrand)) %>%
    unnest(group) %>%
    group_by(N,iter,marginal) %>%
    mutate(species = LETTERS702[1:n()]) %>% 
    ungroup() %>% 
    separate(value, sep = "-G-20", c("filename","junk")) %>% 
    dplyr::select(-junk, -c, -filename)
```

# Functions for Jaccard Distance

## Compare best group structure solution to alternatives

```{r JaccardDistance, message = FALSE, warning = FALSE}
Jaccard_Distance <- function(df){ 
  GroupAll <- tibble()
  df <- randomdf %>%
    mutate(Ni = paste0(N, "_", iter))
  webs <- df %>%
    distinct(Ni) %>%
    pull()
  
  for(w in 1:length(webs)){
    base_df <- df %>% # Base is the "best" grouping as in highest marginal likelihood
      filter(Ni == webs[w]) %>%
      filter(marginal == max(marginal)) %>% 
      group_by(species) %>% 
      slice(1) %>% # If multiple iterations give the same grouping, picking the first one
      ungroup() %>% 
      dplyr::select(species, group) # Best (marginal likelihood) original web group structure to which the reduced networks are compared
 
      time1 <- Sys.time()
      compare_df <- filter(df, Ni == webs[w])
      
      #left_join base_df (X) and compare_df (Y)
      tmp_df <- compare_df %>% 
      dplyr::select(run, species, group) %>%
      left_join(base_df, ., by=c("species")) #%>%

        # Current group to compare from X
        fetchsp.x <- function(g1) {
          base_df %>% 
            filter(group == g1) %>%
            pull(species) %>%
            return()
        }
        
        # Current group to compare from Y
        fetchsp.y <- function(i,g2) {
          tmp_df %>% 
            filter(run == i & group.y == g2) %>%
            pull(species) %>%
            return()
        }
        
            GroupJ <- tmp_df %>% 
              group_by(run, group.x) %>% 
              count(group.y) %>% 
              filter(n == max(n)) %>% 
              slice(1) %>%
              ungroup() %>%  
              mutate(sp.x = pmap(.l = list(group.x), fetchsp.x)) %>%
              mutate(sp.y = pmap(.l = list(run, group.y), fetchsp.y)) %>%
              rowwise() %>%
              mutate(sp.intersect = list(intersect(unlist(sp.x), unlist(sp.y)))) %>%
              mutate(sp.union = list(union(unlist(sp.x), unlist(sp.y)))) %>% 
              mutate(GroupJs = length(unlist(sp.intersect)) / length(unlist(sp.union))) %>%
              ungroup() %>%
              mutate(Ni = webs[w])
            
            GroupAll <- bind_rows(GroupAll, GroupJ)
  
            time2 <- Sys.time()
            dtime <- as.numeric((time2-time1), units = "mins")
            
            message(sprintf("web %s taking %g minutes", webs[w], dtime))
  }
            return(GroupAll) 
}

J_rand_df <- Jaccard_Distance(randomdf) %>%
  group_by(Ni, run) %>%
  summarise(Js = mean(GroupJs))

```

## Random structure similarity (Fig. S3)

```{r}
gg_random_best <- J_rand_df %>%
  separate(Ni, c("N", "iter"), sep = "_") %>%
  mutate(N = case_when(N == "92" ~ "S=92 C=0.049",
          N == "143" ~ "S=143 C=0.086",
          N == "233" ~ "S=233 C=0.042",
          N == "249" ~ "S=249 C=0.053",
          N == "268" ~ "S=268 C=0.023")) %>%
  mutate(N = as.factor(N)) %>%
  mutate(N = fct_relevel(N, c("S=92 C=0.049",
                              "S=143 C=0.086",
                              "S=233 C=0.042",
                              "S=249 C=0.053",
                              "S=268 C=0.023"))) %>%
  ggplot() +
  aes(x = Js) +
  geom_histogram(aes(y = after_stat(count / sum(count))), bins=30) +
  scale_x_continuous(limits = c(0,1), labels = c("0","0.25","0.5","0.75","1")) +
  facet_grid(N~iter, scales="free_y") + 
  labs(x = "Jaccard similarity", y = "Proportion")
gg_random_best
ggsave("../results/figs_random.pdf", gg_random_best, width = 10, height = 7)
```

